{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nDJi-kY7IOX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import mediapipe as mp\n",
        "from app.config import EMBEDDING_SIZE\n",
        "\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True)\n",
        "\n",
        "def get_face_embeddings(image: Image.Image):\n",
        "    img_np = np.array(image)\n",
        "    results = face_mesh.process(cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR))\n",
        "    if not results.multi_face_landmarks:\n",
        "        return torch.zeros((1, EMBEDDING_SIZE))\n",
        "    landmarks = results.multi_face_landmarks[0].landmark\n",
        "    embedding = np.array([[lm.x, lm.y, lm.z] for lm in landmarks[:468]]).flatten()\n",
        "    return torch.tensor(embedding, dtype=torch.float32).unsqueeze(0)\n"
      ]
    }
  ]
}